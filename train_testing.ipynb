{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1782,
     "status": "ok",
     "timestamp": 1597401414218,
     "user": {
      "displayName": "gudipally harshini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgpwKA7wZZVdHt4mJHJbs2DmjGB4MY3CIfLcJfS=s64",
      "userId": "11852060824398980427"
     },
     "user_tz": -330
    },
    "id": "_Pg7GbNvF6Xa",
    "outputId": "93bd2fa6-15c3-4014-bb57-1f315e99a304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1597401416503,
     "user": {
      "displayName": "gudipally harshini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgpwKA7wZZVdHt4mJHJbs2DmjGB4MY3CIfLcJfS=s64",
      "userId": "11852060824398980427"
     },
     "user_tz": -330
    },
    "id": "aLtia2rXGH37",
    "outputId": "8a5d3cf1-d6ee-46b5-f080-a3d4be6f8175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/avantrika/keras-autoencoder-cbir\n"
     ]
    }
   ],
   "source": [
    "cd drive/My Drive/avantrika/keras-autoencoder-cbir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8434,
     "status": "ok",
     "timestamp": 1597401425570,
     "user": {
      "displayName": "gudipally harshini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgpwKA7wZZVdHt4mJHJbs2DmjGB4MY3CIfLcJfS=s64",
      "userId": "11852060824398980427"
     },
     "user_tz": -330
    },
    "id": "u6YjQRu_F4b5"
   },
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from convautoencoder import ConvAutoencoder\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1597401433165,
     "user": {
      "displayName": "gudipally harshini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgpwKA7wZZVdHt4mJHJbs2DmjGB4MY3CIfLcJfS=s64",
      "userId": "11852060824398980427"
     },
     "user_tz": -330
    },
    "id": "ciMc1uyTF4b_"
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(decoded, gt, samples=10):\n",
    "    # initialize our list of output images\n",
    "    outputs = None\n",
    "    gt = np.squeeze(gt, axis = -1)\n",
    "    # loop over our number of output samples\n",
    "    for i in range(0, samples):\n",
    "      # print(gt[i])\n",
    "      # print(gt[i].shape, decoded[i].shape)\n",
    "      # grab the original image and reconstructed image\n",
    "      original = (gt[i] * 255).astype(\"uint8\")\n",
    "      recon = (decoded[i] * 255).astype(\"uint8\")\n",
    "      # stack the original and reconstructed image side-by-side\n",
    "      output = np.hstack([original, recon])\n",
    "      # print(output)\n",
    "      # if the outputs array is empty, initialize it as the current\n",
    "      # side-by-side image display\n",
    "      if outputs is None:\n",
    "          outputs = output\n",
    "\n",
    "      # otherwise, vertically stack the outputs\n",
    "      else:\n",
    "          outputs = np.vstack([outputs, output])\n",
    "\n",
    "    # return the output images\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1597401434048,
     "user": {
      "displayName": "gudipally harshini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgpwKA7wZZVdHt4mJHJbs2DmjGB4MY3CIfLcJfS=s64",
      "userId": "11852060824398980427"
     },
     "user_tz": -330
    },
    "id": "bxMkC_IrF4cE"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_path = \"/home/harshini/workspace/keras-autoencoder-cbir-20200908T054132Z-001/keras-autoencoder-cbir/dataset/output/autoencoder.h5\"\n",
    "vis_path = \"/home/harshini/workspace/keras-autoencoder-cbir-20200908T054132Z-001/keras-autoencoder-cbir/dataset/output/recon_vis.png\"\n",
    "plot_path = \"/home/harshini/workspace/keras-autoencoder-cbir-20200908T054132Z-001/keras-autoencoder-cbir/dataset/output/plot.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySGI_InPn-Qd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples 101\n",
      "(101, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "IMAGES_FOLDER = '/home/harshini/workspace/keras-autoencoder-cbir-20200908T054132Z-001/keras-autoencoder-cbir/dataset/train'\n",
    "OUTPUT = '/home/harshini/workspace/keras-autoencoder-cbir-20200908T054132Z-001/keras-autoencoder-cbir/dataset/output'\n",
    "\n",
    "### Initialise empty numpy arrays\n",
    "\n",
    "data = np.empty((0,512,512,3), dtype=np.int8)\n",
    "\n",
    "### Read annotation file, fetch image, normalise image and array, compose data and target arrays\n",
    "\n",
    "for root, folders, files in os.walk(IMAGES_FOLDER):\n",
    "    for file in files:\n",
    "        image_path = os.path.join(IMAGES_FOLDER, file)\n",
    "        image = cv2.imread(image_path)/255\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        if image is not None:\n",
    "            data = np.vstack((data, image))\n",
    "\n",
    "\n",
    "### Shuffle data and target synchronously\n",
    "\n",
    "num_samples = data.shape[0]\n",
    "arr = np.arange(num_samples)\n",
    "np.random.shuffle(arr)\n",
    "print(\"num_samples\", num_samples)\n",
    "data = data[arr]\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "np.save(os.path.join(OUTPUT,'train.npy'), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 602574,
     "status": "ok",
     "timestamp": 1597402064410,
     "user": {
      "displayName": "gudipally harshini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgpwKA7wZZVdHt4mJHJbs2DmjGB4MY3CIfLcJfS=s64",
      "userId": "11852060824398980427"
     },
     "user_tz": -330
    },
    "id": "L7ZYJdzKoD2K",
    "outputId": "64580901-4941-48ad-a4aa-089f50637ef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples 25\n",
      "(25, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMAGES_FOLDER = '/home/harshini/workspace/keras-autoencoder-cbir-20200908T054132Z-001/keras-autoencoder-cbir/dataset/test'\n",
    "OUTPUT = '/home/harshini/workspace/keras-autoencoder-cbir-20200908T054132Z-001/keras-autoencoder-cbir/dataset/output'\n",
    "\n",
    "### Initialise empty numpy arrays\n",
    "\n",
    "data = np.empty((0,512,512,3), dtype=np.int8)\n",
    "\n",
    "### Read annotation file, fetch image, normalise image and array, compose data and target arrays\n",
    "\n",
    "for root, folders, files in os.walk(IMAGES_FOLDER):\n",
    "    for file in files:\n",
    "        image_path = os.path.join(IMAGES_FOLDER, file)\n",
    "        image = cv2.imread(image_path)/255\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        if image is not None:\n",
    "            data = np.vstack((data, image))\n",
    "\n",
    "\n",
    "### Shuffle data and target synchronously\n",
    "\n",
    "num_samples = data.shape[0]\n",
    "arr = np.arange(num_samples)\n",
    "np.random.shuffle(arr)\n",
    "print(\"num_samples\", num_samples)\n",
    "data = data[arr]\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "np.save(os.path.join(OUTPUT,'test.npy'), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1597400896915,
     "user": {
      "displayName": "gudipally harshini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgpwKA7wZZVdHt4mJHJbs2DmjGB4MY3CIfLcJfS=s64",
      "userId": "11852060824398980427"
     },
     "user_tz": -330
    },
    "id": "dHZIb5_xF4cJ"
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs to train for, initial learning rate,\n",
    "# and batch size\n",
    "EPOCHS = 5\n",
    "INIT_LR = 1e-3\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 642,
     "status": "error",
     "timestamp": 1597401056163,
     "user": {
      "displayName": "gudipally harshini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgpwKA7wZZVdHt4mJHJbs2DmjGB4MY3CIfLcJfS=s64",
      "userId": "11852060824398980427"
     },
     "user_tz": -330
    },
    "id": "kjd6UTxvF4cU",
    "outputId": "89771d3f-a875-4b78-9454-4b4210343380"
   },
   "outputs": [],
   "source": [
    "\n",
    "trainX = np.load('/home/harshini/workspace/keras-autoencoder-cbir-20200908T054132Z-001/keras-autoencoder-cbir/dataset/output/train.npy')\n",
    "testX = np.load('/home/harshini/workspace/keras-autoencoder-cbir-20200908T054132Z-001/keras-autoencoder-cbir/dataset/output/test.npy')\n",
    "# add a channel dimension to every image in the dataset, then scale\n",
    "# the pixel intensities to the range [0, 1]\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yA-yB1U0F4cZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building autoencoder...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (101, 512, 512, 3, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-11017dae22aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     batch_size=BS)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    563\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (101, 512, 512, 3, 1)"
     ]
    }
   ],
   "source": [
    "# construct our convolutional autoencoder\n",
    "print(\"[INFO] building autoencoder...\")\n",
    "autoencoder = ConvAutoencoder.build(512, 512, 3)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
    "\n",
    "# train the convolutional autoencoder\n",
    "H = autoencoder.fit(\n",
    "    trainX, trainX,\n",
    "    validation_data=(testX, testX),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEWFDJjFF4cm"
   },
   "outputs": [],
   "source": [
    "\n",
    "# use the convolutional autoencoder to make predictions on the\n",
    "# testing images, construct the visualization, and then save it\n",
    "# to disk\n",
    "print(\"[INFO] making predictions...\")\n",
    "decoded = autoencoder.predict(testX)\n",
    "# print(decoded)\n",
    "vis = visualize_predictions(decoded, testX)\n",
    "cv2.imwrite(vis_path, vis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_4muH7-HLoa"
   },
   "outputs": [],
   "source": [
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(plot_path)\n",
    "\n",
    "# serialize the autoencoder model to disk\n",
    "print(\"[INFO] saving autoencoder...\")\n",
    "autoencoder.save(model_path, save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqVGkMoyJpXP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4536,
     "status": "ok",
     "timestamp": 1597345977122,
     "user": {
      "displayName": "gudipally harshini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgpwKA7wZZVdHt4mJHJbs2DmjGB4MY3CIfLcJfS=s64",
      "userId": "11852060824398980427"
     },
     "user_tz": -330
    },
    "id": "f9fAWKmgPLVp",
    "outputId": "f28d8123-09f6-4294-99d1-5e54f8dcd509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST training split...\n",
      "[INFO] loading autoencoder model...\n",
      "[INFO] encoding images...\n",
      "[INFO] saving index...\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python index_images.py --model output/autoencoder.h5 --index output/features.pickle\n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "model_path = \"/content/drive/My Drive/avantrika/keras-autoencoder-cbir/dataset/output/autoencoder.h5\"\n",
    "index_path = \"/content/drive/My Drive/avantrika/keras-autoencoder-cbir/dataset/output/features.pickle\"\n",
    "\n",
    "# load the MNIST dataset\n",
    "print(\"[INFO] loading MNIST training split...\")\n",
    "\n",
    "trainX = np.load('/content/drive/My Drive/avantrika/keras-autoencoder-cbir/dataset/output/train.npy')\n",
    "testX = np.load('/content/drive/My Drive/avantrika/keras-autoencoder-cbir/dataset/output/test.npy')\n",
    "\n",
    "# add a channel dimension to every image in the training split, then\n",
    "# scale the pixel intensities to the range [0, 1]\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "\n",
    "# load our autoencoder from disk\n",
    "print(\"[INFO] loading autoencoder model...\")\n",
    "autoencoder = load_model(model_path)\n",
    "\n",
    "# create the encoder model which consists of *just* the encoder\n",
    "# portion of the autoencoder\n",
    "encoder = Model(inputs=autoencoder.input,\n",
    "\toutputs=autoencoder.get_layer(\"encoded\").output)\n",
    "\n",
    "# quantify the contents of our input images using the encoder\n",
    "print(\"[INFO] encoding images...\")\n",
    "features = encoder.predict(trainX)\n",
    "\n",
    "# construct a dictionary that maps the index of the MNIST training\n",
    "# image to its corresponding latent-space representation\n",
    "indexes = list(range(0, trainX.shape[0]))\n",
    "data = {\"indexes\": indexes, \"features\": features}\n",
    "\n",
    "# write the data dictionary to disk\n",
    "print(\"[INFO] saving index...\")\n",
    "f = open(index_path, \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98vAw3ZIPLSx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8294,
     "status": "ok",
     "timestamp": 1597348020848,
     "user": {
      "displayName": "gudipally harshini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgpwKA7wZZVdHt4mJHJbs2DmjGB4MY3CIfLcJfS=s64",
      "userId": "11852060824398980427"
     },
     "user_tz": -330
    },
    "id": "FQEr7BwIPgyB",
    "outputId": "b1527a2f-e383-4fd1-acb2-6ac9174d4860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading dataset...\n",
      "[INFO] loading autoencoder and index...\n",
      "[INFO] encoding testing images...\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f351ce59378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python search.py --model output/autoencoder.h5 --index output/index.pickle \n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from imutils import build_montages\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "\n",
    "def euclidean(a, b):\n",
    "\t# compute and return the euclidean distance between two vectors\n",
    "\treturn np.linalg.norm(a - b)\n",
    "\n",
    "def perform_search(queryFeatures, index, maxResults=64):\n",
    "\t# initialize our list of results\n",
    "\tresults = []\n",
    "\n",
    "\t# loop over our index\n",
    "\tfor i in range(0, len(index[\"features\"])):\n",
    "\t\t# compute the euclidean distance between our query features\n",
    "\t\t# and the features for the current image in our index, then\n",
    "\t\t# update our results list with a 2-tuple consisting of the\n",
    "\t\t# computed distance and the index of the image\n",
    "\t\td = euclidean(queryFeatures, index[\"features\"][i])\n",
    "\t\tresults.append((d, i))\n",
    "\n",
    "\t# sort the results and grab the top ones\n",
    "\tresults = sorted(results)[:maxResults]\n",
    "\n",
    "\t# return the list of results\n",
    "\treturn results\n",
    "\n",
    "\n",
    "model_path = \"/content/drive/My Drive/avantrika/keras-autoencoder-cbir/dataset/output/autoencoder.h5\"\n",
    "index_path = \"/content/drive/My Drive/avantrika/keras-autoencoder-cbir/dataset/output/features.pickle\"\n",
    "sample = 10\n",
    "\n",
    "# load the MNIST dataset\n",
    "print(\"[INFO] loading dataset...\")\n",
    "trainX = np.load('/content/drive/My Drive/avantrika/keras-autoencoder-cbir/dataset/output/train.npy')\n",
    "testX = np.load('/content/drive/My Drive/avantrika/keras-autoencoder-cbir/dataset/output/test.npy')\n",
    "\n",
    "# add a channel dimension to every image in the dataset, then scale\n",
    "# the pixel intensities to the range [0, 1]\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "\n",
    "# load the autoencoder model and index from disk\n",
    "print(\"[INFO] loading autoencoder and index...\")\n",
    "autoencoder = load_model(model_path)\n",
    "index = pickle.loads(open(index_path, \"rb\").read())\n",
    "\n",
    "# create the encoder model which consists of *just* the encoder\n",
    "# portion of the autoencoder\n",
    "encoder = Model(inputs=autoencoder.input,\n",
    "\toutputs=autoencoder.get_layer(\"encoded\").output)\n",
    "\n",
    "# quantify the contents of our input testing images using the encoder\n",
    "print(\"[INFO] encoding testing images...\")\n",
    "features = encoder.predict(testX)\n",
    "\n",
    "# randomly sample a set of testing query image indexes\n",
    "queryIdxs = list(range(0, testX.shape[0]))\n",
    "queryIdxs = np.random.choice(queryIdxs, size=sample,\n",
    "\treplace=False)\n",
    "\n",
    "trainX = np.squeeze(trainX, axis = -1)\n",
    "testX = np.squeeze(testX, axis = -1)\n",
    "\n",
    "# loop over the testing indexes\n",
    "for i in queryIdxs:\n",
    "  # take the features for the current image, find all similar\n",
    "  # images in our dataset, and then initialize our list of result\n",
    "  # images\n",
    "  queryFeatures = features[i]\n",
    "  results = perform_search(queryFeatures, index, maxResults=225)\n",
    "  images = []\n",
    "\n",
    "  # loop over the results\n",
    "  for (d, j) in results:\n",
    "    # grab the result image, convert it back to the range\n",
    "    # [0, 255], and then update the images list\n",
    "    image = (trainX[j] * 255).astype(\"uint8\")\n",
    "    # image = np.dstack([image] * 3)\n",
    "    images.append(image)\n",
    "\n",
    "  # display the query image\n",
    "  query = (testX[i] * 255).astype(\"uint8\")\n",
    "  # cv2.imshow(\"Query\", query)\n",
    "  cv2.imwrite(\"/content/drive/My Drive/avantrika/keras-autoencoder-cbir/dataset/output/smoke_testing/\" + str(j) + \".jpg\", query)\n",
    "\n",
    "  # build a montage from the results and display it\n",
    "  montage = build_montages(images, (512, 512), (10, 10))[0]\n",
    "  cv2.imwrite(\"/content/drive/My Drive/avantrika/keras-autoencoder-cbir/dataset/output/smoke_testing/montage/\" + str(j) + \".jpg\", montage)\n",
    "  # cv2.imshow(\"Results\", montage)\n",
    "  # cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NCuM1kHR8Q4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_testing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
